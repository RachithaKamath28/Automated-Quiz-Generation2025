module 1 1 chapter graphics systems and models perhapsthedominantcharacteristicofthisnewmillenniumishowcomputerand communicationtechnologieshavebecomedominantforcesinourlives.activi- tiesaswide-rangingasfilmmaking,publishing,banking,andeducationcontinueto undergorevolutionarychangesasthesetechnologiesalterthewaysinwhichwecon- ductourdailyactivities.thecombinationofcomputers,networks,andthecomplex humanvisualsystem,throughcomputergraphics,hasledtonewwaysofdisplaying information,seeingvirtualworlds,andcommunicatingwithpeopleandmachines. computergraphicsisconcernedwithallaspectsofproducingpicturesorim- agesusingacomputer.thefieldbeganhumblyalmost50yearsago,withthedisplay ofafewlinesonacathode-raytube(crt);now,wecancreateimagesbycomputer that are indistinguishable from photographs of real objects. we routinely train pi- lotswithsimulatedairplanes,generatinggraphicaldisplaysofavirtualenvironment inrealtime.feature-lengthmoviesmadeentirelybycomputerhavebeensuccessful, bothcriticallyandfinancially.massivemultiplayergamescaninvolvetensofthou- sandsofconcurrentparticipants. inthischapter, westartourjourneywithashortdiscussionofapplicationsof computer graphics. then we overview graphics systems and imaging. throughout this book, our approach stresses the relationships between computer graphics and imageformationbyfamiliarmethods,suchasdrawingbyhandandphotography.we willseethattheserelationshipscanhelpustodesignapplicationprograms,graphics libraries,andarchitecturesforgraphicssystems. inthisbook,weintroduceaparticulargraphicssoftwaresystem,opengl,which hasbecomeawidelyacceptedstandardfordevelopinggraphicsapplications.fortu- nately,opengliseasytolearn,anditpossessesmostofthecharacteristicsofother populargraphicssystems.ourapproachistop-down.wewantyoutostartwriting, asquicklyaspossible,applicationprogramsthatwillgenerategraphicaloutput.af- teryoubeginwritingsimpleprograms,wewilldiscusshowtheunderlyinggraphics libraryandthehardwareareimplemented.thischaptershouldgiveasufficientover- viewforyoutoproceedtowritingprograms. 1 2 chapter1 graphicssystemsandmodels 1.1 applications of computer graphics thedevelopmentofcomputergraphicshasbeendrivenbothbytheneedsoftheuser communityandbyadvancesinhardwareandsoftware.theapplicationsofcomputer graphicsaremanyandvaried;wecan,however,dividethemintofourmajorareas: 1. displayofinformation 2. design 3. simulationandanimation 4. userinterfaces althoughmanyapplicationsspantwoormoreoftheseareas,thedevelopmentofthe fieldwasbasedonseparateworkineach. 1.1.1 display of information classicalgraphicstechniquesaroseasamediumtoconveyinformationamongpeo- ple.althoughspokenandwrittenlanguagesserveasimilarpurpose,thehumanvi- sualsystemisunrivaledbothasaprocessorofdataandasapatternrecognizer.more than 4000 years ago, the babylonians displayed floor plans of buildings on stones. more than 2000 years ago, the greeks were able to convey their architectural ideas graphically, even though the related mathematics was not developed until the re- naissance.today,thesametypeofinformationisgeneratedbyarchitects,mechanical designers,anddraftspeopleusingcomputer-baseddraftingsystems. forcenturies, cartographershavedevelopedmapstodisplaycelestialandgeo- graphicalinformation.suchmapswerecrucialtonavigatorsasthesepeopleexplored theendsoftheearth;mapsarenolessimportanttodayinfieldssuchasgeographic informationsystems.now,mapscanbedevelopedandmanipulatedinrealtimeover theinternet. overthepast150years,workersinthefieldofstatisticshaveexploredtechniques for generating plots that aid the viewer in understanding the information in a set ofdata.now,wehavecomputerplottingpackagesthatprovideavarietyofplotting techniquesandcolortoolsthatcanhandlemultiplelargedatasets.nevertheless, it is still the humans ability to recognize visual patterns that ultimately allows us to interprettheinformationcontainedinthedata.thefieldofinformationvisualiza- tionisbecomingincreasinglymoreimportantaswehavetodealwithunderstanding complexphenomenafromproblemsinbioinformaticstodetectingsecuritythreats. medicalimagingposesinterestingandimportantdata-analysisproblems.mod- ernimagingtechnologiessuchascomputedtomography(ct),magneticresonance imaging (mri), ultrasound, and positron-emission tomography (pet)generate three-dimensionaldatathatmustbesubjectedtoalgorithmicmanipulationtopro- videusefulinformation.colorplate20showsanimageofapersonsheadinwhich the skin is displayed as transparent and the internal structures are displayed as opaque. although the data were collected by a medical imaging system, computer graphicsproducedtheimagethatshowsthestructuralinformation. 1.1 applicationsofcomputergraphics 3 supercomputers now allow researchers in many areas to solve previously in- tractableproblems.thefieldofscientificvisualizationprovidesgraphicaltoolsthat helptheseresearchersinterpretthevastquantityofdatathattheygenerate.infields suchasfluidflow,molecularbiology,andmathematics,imagesgeneratedbyconver- sionofdatatogeometricentitiesthatcanbedisplayedhaveyieldednewinsightsinto complexprocesses.forexample,colorplate19showsfluiddynamicsinthemantle oftheearth.thesystemusedamathematicalmodeltogeneratethedata.thefieldof informationvisualizationusescomputergraphicstoaidinthediscoveryofrelation- shipsindatasetsinwhichthereisnophysicaltiebetweenthedataandhowtheyare visualized.wepresentvariousvisualizationtechniquesasexamplesthroughoutthe restofthetext. 1.1.2 design professionssuchasengineeringandarchitectureareconcernedwithdesign.starting withasetofspecifications,engineersandarchitectsseekacost-effectiveandesthetic solutionthatsatisfiesthespecifications.designisaniterativeprocess.rarelyinthe realworldisaproblemspecifiedsuchthatthereisauniqueoptimalsolution.design problemsareeitheroverdetermined,suchthattheypossessnosolutionthatsatisfies all the criteria, much less an optimal solution, or underdetermined, such that they have multiple solutions that satisfy the design criteria. thus, the designer works iteratively. she generates a possible design, tests it, and then uses the results as the basisforexploringothersolutions. thepoweroftheparadigmofhumansinteractingwithimagesonthescreenof acrtwasrecognizedbyivansutherlandmorethan40yearsago.today,theuseof interactivegraphicaltoolsincomputer-aideddesign(cad)pervadesfieldsinclud- ingasarchitecture,mechanicalengineering,thedesignofvery-large-scaleintegrated (vlsi) circuits, and the creation of characters for animations. in all these applica- tions, the graphics are used in a number of distinct ways. for example, in a vlsi design,thegraphicsprovideaninteractiveinterfacebetweentheuserandthedesign package, usually by means of such tools as menus and icons. in addition, after the userproducesapossibledesign,othertoolsanalyzethedesignanddisplaytheanaly- sisgraphically.colorplates9and10showtwoviewsofthesamearchitecturaldesign. bothimagesweregeneratedwiththesamecadsystem.theydemonstratetheim- portanceofhavingthetoolsavailabletogeneratedifferentimagesofthesameobjects atdifferentstagesofthedesignprocess. 1.1.3 simulation and animation once graphics systems evolved to be capable of generating sophisticated images in real time, engineers and researchers began to use them as simulators. one of the most important uses has been in the training of pilots. graphical flight simula- torshaveprovedtoincreasesafetyandtoreducetrainingexpenses.theuseofspe- cial vlsi chips has led to a generation of arcade games as sophisticated as flight simulators. 4 chapter1 graphicssystemsandmodels gamesandeducationalsoftwareforhomecomputersarealmostasimpressive. colorplate16showsaphysicalrobotandthecorrespondinggraphicalsimulation. thesimulatorcanbeusedfordesigningtherobot,planningitspath,andsimulating itsbehaviorincomplexenvironments. thesuccessofflightsimulatorsledtotheuseofcomputergraphicsforanima- tion in the television, motion picture, and advertising industries. entire animated moviescannowbemadebycomputersatacostlessthanthatofmoviesmadewith traditionalhand-animationtechniques.theuseofcomputergraphicswithhandan- imationallowsthecreationoftechnicalandartisticeffectsthatarenotpossiblewith eitheralone.whereascomputeranimationshaveadistinctlook,wecanalsogenerate photorealisticimagesbycomputer.oftenimagesthatweseeontelevision,inmovies, andinmagazinesaresorealisticthatwecannotdistinguishcomputer-generatedor computer-alteredimagesfromphotographs.inchapters6and9, wediscussmany of the lighting effects used to produce computer animations. color plate 13 shows a scene from a computer-generated video. the artists and engineers who created thissceneusedcommerciallyavailablesoftware.theplatedemonstratesourability togeneraterealisticenvironments,inthiscaseaweldingrobotinsideafactory.the sparksdemonstratetheuseofproceduralmethodsforspecialeffects.wewilldiscuss thesetechniquesinchapter11.theimagesincolorplate31showanotherexample oftheuseofcomputergraphicstogenerateaneffectthat,althoughitlooksrealistic, couldnothavebeencreatedotherwise.theimagesincolorplates23and24alsoare realisticrenderings. thefieldofvirtualreality(vr)hasopenedmanynewhorizons.ahumanviewer canbeequippedwithadisplayheadsetthatallowshertoseeseparateimageswithher righteyeandherlefteye, whichgivestheeffectofstereoscopicvision.inaddition, herbodylocationandposition,possiblyincludingherheadandfingerpositions,are trackedbythecomputer.shemayhaveotherinteractivedevicesavailable,including force-sensing gloves and sound. she can then act as part of a computer-generated scene, limited only by the image-generation ability of the computer. for example, a surgical intern might be trained to do an operation in this way, or an astronaut might be trained to work in a weightless environment. color plate 22 shows one frameofavrsimulationofasimulatedpatientusedforremotetrainingofmedical personnel. thegraphicstodriveinteractivevideogamesmakeheavyuseofbothstandard commodity computers and specialized hardware boxes. to a large degree, games drive the development of graphics hardware. on the commercial side, the revenue from video games has surpassed the revenue for commercial films. the graphics technologyforgames,bothintheformofthegraphicsprocessingunitsthatareon graphicscardsinpersonalcomputersandingameboxessuchasthexboxandthe playstation,isbeingusedforsimulationratherthanexpensivespecializedhardware. color plate 16 is a frame from an interactive game, showing a robot warrior, that uses hierarchical methods (chapter 10), procedural methods for smoke and fire (chapter11),andnoisetexturesforlandscapes(chapter11).theenginethatdrives thegameusesscenegraphsthatwepresentinchapter10. 1.2 agraphicssystem 5 1.1.4 user interfaces our interaction with computers has become dominated by a visual paradigm that includes windows, icons, menus, and a pointing device, such as a mouse. from a users perspective, windowing systems such as the x window system, microsoft windows, and the macintosh os x differ only in details. more recently, millions of people have become internet users. their access is through graphical network browsers,suchasfirefoxandinternetexplorer,thatusethesesameinterfacetools. wehavebecomesoaccustomedtothisstyleofinterfacethatweoftenforgetthatwe areworkingwithcomputergraphics. althoughwearefamiliarwiththestyleofgraphicaluserinterfaceusedonmost workstations,1advancesincomputergraphicshavemadepossibleotherformsofin- terfaces.colorplate14showstheinterfaceusedwithahigh-levelmodelingpackage. itdemonstratesthevarietyofthetoolsavailableinsuchpackagesandtheinteractive devicestheusercanemployinmodelinggeometricobjects. 1.2 a graphics system a computer graphics system is a computer system; as such, it must have all the componentsofageneral-purposecomputersystem.letusstartwiththehigh-level viewofagraphicssystem,asshownintheblockdiagraminfigure1.1.therearefive majorelementsinoursystem: 1. inputdevices 2. processor 3. memory 4. framebuffer 5. outputdevices thismodelisgeneralenoughtoincludeworkstationsandpersonalcomputers,inter- active game systems, and sophisticated image-generation systems. although all the components, withthepossibleexceptionoftheframebuffer, arepresentinastan- dardcomputer,itisthewayeachelementisspecializedforcomputergraphicsthat characterizesthisdiagramasaportraitofagraphicssystem. 1.2.1 pixels and the frame buffer presently, almost all graphics systems are raster based. a picture is produced as an arraythe rasterof picture elements, or pixels, within the graphics system. as we can see from figure 1.2, each pixel corresponds to a location, or small area, in 1.althoughpersonalcomputersandworkstationsevolvedbysomewhatdifferentpaths,presently, thereisvirtuallynofundamentaldifferencebetweenthem.hence,weusethetermspersonalcom- puterandworkstationsynonymously. 6 chapter1 graphicssystemsandmodels frame processor buffer memory figure1.1 agraphicssystem. figure1.2 pixels.(a)imageofyetithecat.(b)detailofareaaround oneeyeshowingindividualpixels. the image. collectively, the pixels are stored in a part of memory called the frame buffer.theframebuffercanbeviewedasthecoreelementofagraphicssystem.its resolutionthe number of pixels in the frame bufferdetermines the detail that youcanseeintheimage.thedepth,orprecision,oftheframebuffer,definedasthe numberofbitsthatareusedforeachpixel,determinespropertiessuchashowmany colorscanberepresentedonagivensystem.forexample,a1-bit-deepframebuffer allows only two colors, whereas an 8-bit-deep frame buffer allows 28 (256) colors. infull-colorsystems,thereare24(ormore)bitsperpixel.suchsystemscandisplay sufficientcolorstorepresentmostimagesrealistically.theyarealsocalledtrue-color systems, or rgb-color systems, because individual groups of bits in each pixel are 1.2 agraphicssystem 7 assigned to each of the three primary colorsred, green, and blueused in most displays. highdynamicrangeapplicationsrequiremorethan24-bitfixedpointcolorrep- resentationsofrgbcolors.somerecentframebuffersstorergbvaluesasfloating- pointnumbersinstandardieeeformat.hence,thetermtruecolor shouldbeinter- pretedasframebuffersthathavesufficientdepthtorepresentcolorsintermsofrgb valuesratherthanasindicesintoalimitedsetofcolors.wewillreturntothistopic inchapter2. theframebufferusuallyisimplementedwithspecialtypesofmemorychipsthat enablefastredisplayofthecontentsoftheframebuffer.insoftware-basedsystems, such as those used for high-resolution rendering or for generating complex visual effects that cannot be produced in real time, the frame buffer is part of system memory. inaverysimplesystem,theframebufferholdsonlythecoloredpixelsthatare displayedonthescreen.inmostsystems,theframebufferholdsfarmoreinforma- tion,suchasdepthinformationneededforcreatingimagesfromthree-dimensional data.inthesesystems,theframebuffercomprisesmultiplebuffers,oneormoreof whicharecolorbuffersthatholdthecoloredpixelsthataredisplayed.fornow,we canusethetermsframebufferandcolorbuffersynonymouslywithoutconfusion. inasimplesystem,theremaybeonlyoneprocessor,thecentralprocessingunit (cpu)ofthesystem,whichmustdoboththenormalprocessingandthegraphical processing.themaingraphicalfunctionoftheprocessoristotakespecificationsof graphical primitives (such as lines, circles, and polygons) generated by application programs and to assign values to the pixels in the frame buffer that best represent theseentities.forexample,atriangleisspecifiedbyitsthreevertices,buttodisplay its outline by the three line segments connecting the vertices, the graphics system mustgenerateasetofpixelsthatappearaslinesegmentstotheviewer.theconver- sionofgeometricentitiestopixelcolorsandlocationsintheframebufferisknown asrasterization,orscanconversion.inearlygraphicssystems,theframebufferwas partofthestandardmemorythatcouldbedirectlyaddressedbythecpu.today,vir- tuallyallgraphicssystemsarecharacterizedbyspecial-purposegraphicsprocessing units(gpus),custom-tailoredtocarryoutspecificgraphicsfunctions.thegpucan beeitheronthemotherboardofthesystemoronagraphicscard.theframebuffer isaccessedthroughthegraphicsprocessingunitandmaybeincludedinthegpu. 1.2.2 output devices for many years, the dominant type of display (or monitor) has been the cathode- ray tube (crt). although various flat-panel technologies are now more popular, the basic functioning of the crt has much in common with these newer displays. a simplified picture of a crt is shown in figure 1.3. when electrons strike the phosphorcoatingonthetube,lightisemitted.thedirectionofthebeamiscontrolled bytwopairsofdeflectionplates.theoutputofthecomputerisconverted,bydigital- to-analogconverters, tovoltagesacrossthex andy deflectionplates.lightappears 8 chapter1 graphicssystemsandmodels y deflect electron gun x deflect phosphor focus figure1.3 thecathode-raytube(crt). onthesurfaceofthecrtwhenasufficientlyintensebeamofelectronsisdirectedat thephosphor. ifthevoltagessteeringthebeamchangeataconstantrate, thebeamwilltrace a straight line, visible to a viewer. such a device is known as the random-scan, calligraphic, or vector crt, because the beam can be moved directly from any positiontoanyotherposition.iftheintensityofthebeamisturnedoff,thebeamcan bemovedtoanewpositionwithoutchanginganyvisibledisplay.thisconfiguration wasthebasisofearlygraphicssystemsthatpredatedthepresentrastertechnology. atypicalcrtwillemitlightforonlyashorttimeusually,afewmilliseconds after the phosphor is excited by the electron beam. for a human to see a steady, flicker-free image on most crt displays, the same path must be retraced, or re- freshed, by the beam at a sufficiently high rate, the refreshrate. in older systems, therefreshrateisdeterminedbythefrequencyofthepowersystem,60cyclespersec- ondor60hertz(hz)intheunitedstatesand50hzinmuchoftherestoftheworld. moderndisplaysarenolongercoupledtotheselowfrequenciesandoperateatrates uptoabout85hz. in a raster system, the graphics system takes pixels from the frame buffer and displaysthemaspointsonthesurfaceofthedisplayinoneoftwofundamentalways. in a noninterlaced or progressive display, the pixels are displayed row by row, or scanlinebyscanline,attherefreshrate.inaninterlaceddisplay,oddrowsandeven rowsarerefreshedalternately.interlaceddisplaysareusedincommercialtelevision. inaninterlaceddisplayoperatingat60hz,thescreenisredrawninitsentiretyonly 30timespersecond, althoughthevisualsystemistrickedintothinkingtherefresh rateis60hzratherthan30hz.viewerslocatednearthescreen,however,cantellthe differencebetweentheinterlacedandnoninterlaceddisplays.noninterlaceddisplays arebecomingmorewidespread,eventhoughthesedisplaysprocesspixelsattwicethe rateofinterlaceddisplays. color crts have three different colored phosphors (red, green, and blue), ar- ranged in small groups. one common style arranges the phosphors in triangular groupscalledtriads,eachtriadconsistingofthreephosphors,oneofeachprimary. 1.2 agraphicssystem 9 blue gun triad green red green gun blue red gun shadow mask figure1.4 shadow-maskcrt. vertical grid light emitting elements horizontal grid figure1.5 genericflat-paneldisplay. mostcolorcrtshavethreeelectronbeams,correspondingtothethreetypesofphos- phors.intheshadow-maskcrt(figure1.4),ametalscreenwithsmallholesthe shadowmaskensuresthatanelectronbeamexcitesonlyphosphorsoftheproper color. although crts are still the most common display device, they are rapidly be- ing replaced by flat-screen technologies. flat-panel monitors are inherently raster. although there are multiple technologies available, including light-emitting diodes (leds),liquid-crystaldisplays(lcds),andplasmapanels,alluseatwo-dimensional grid to address individual light-emitting elements. figure 1.5 shows a generic flat- panelmonitor.thetwooutsideplatescontainparallelgridsofwiresthatareoriented perpendiculartoeachother.bysendingelectricalsignalstotheproperwireineach grid,theelectricalfieldatalocation,determinedbytheintersectionoftwowires,can be made strong enough to control the corresponding element in the middle plate. themiddleplateinanledpanelcontainslight-emittingdiodesthatcanbeturned onandoffbytheelectricalsignalssenttothegrid.inanlcddisplay,theelectrical fieldcontrolsthepolarizationoftheliquidcrystalsinthemiddlepanel,thusturning onandoffthelightpassingthroughthepanel.aplasmapanelusesthevoltageson thegridstoenergizegasesembeddedbetweentheglasspanelsholdingthegrids.the energizedgasbecomesaglowingplasma. 10 chapter1 graphicssystemsandmodels most projection systems are also raster devices. these systems use a variety of technologies,includingcrtsanddigitallightprojection(dlp).fromauserperspec- tive,theyactasstandardmonitorswithsimilarresolutionsandprecisions.hard-copy devices,suchasprintersandplotters,arealsorasterbasedbutcannotberefreshed. until recently, most displays had a 4:3 width to height ratio (or aspectratio) thatcorrespondedstocommercialtelevision.indiscreteterms,displaysstartedwith vgaresolutionof640480pixels,whichwasconsistentwiththenumberoflines displayedinstandardntscvideo.2computerdisplaysmoveduptothepopularres- olutionsof1024768(xga)and12801024(sxga).thenewerhighdefinition television(hdtv)standardusesa16:9aspectratio,whichisbetweentheoldertele- visionaspectratioandthatofmovies.hdtvmonitorsdisplay780or1080linesin eitherprogressive(1080p,780p)orinterlaced(1080i,780i)modes.hence,themost popularcomputerdisplayresolutionsarenow19201080and1280720,which havethehdtvaspectratio,and19201024and1280768,whichhavetheverti- calresolutionofxgaandsxgadisplays.atthehighend,therearenow4k(4096 2160)digitalprojectorsthataresuitableforcommercialdigitalmovies. 1.2.3 input devices mostgraphicssystemsprovideakeyboardandatleastoneotherinputdevice.the most common input devices are the mouse, the joystick, and the data tablet. each providespositionalinformationtothesystem,andeachusuallyisequippedwithone ormorebuttonstoprovidesignalstotheprocessor.oftencalledpointingdevices, thesedevicesallowausertoindicateaparticularlocationonthedisplay.westudy thesedevicesinchapter3. gameconsoleslackkeyboardsbutincludeagreatervarietyofinputdevicesthan a standard workstation. a typical console might have multiple buttons, a joystick, anddials.devicessuchasthenintendowiiarewirelessandcansenseaccelerations inthreedimensions. games, cad, and virtual reality applications have all generated the need for inputdevicesthatprovidemorethantwo-dimensionaldata.three-dimensionallo- cationsonareal-worldobjectcanbeobtainedbyavarietyofdevices,includinglaser rangefindersandacousticsensors.higher-dimensionaldatacanbeobtainedbyde- vicessuchasdatagloves,whichincludemanysensors,andcomputervisionsystems. 1.3 images: physical and synthetic the traditional pedagogical approach to teaching computer graphics has been fo- cused on how to construct raster images of simple two-dimensional geometric en- tities (for example, points, line segments, and polygons) in the frame buffer. next, most textbooks discussed how to define two- and three-dimensional mathematical 2.outsidetheunitedstates,thepalandsecamsystemsdisplaymorelinesbutusealowerframe rate. 1.3 images:physicalandsynthetic 11 objectsinthecomputerandimagethemwiththesetoftwo-dimensionalrasterized primitives. thisapproachworkedwellforcreatingsimpleimagesofsimpleobjects.inmod- ernsystems,however,wewanttoexploitthecapabilitiesofthesoftwareandhardware to create realistic images of computer-generated three-dimensional objectsa task thatinvolvesmanyaspectsofimageformation,suchaslighting,shading,andprop- ertiesofmaterials.becausesuchfunctionalityissupporteddirectlybymostpresent computergraphicssystems,weprefertosetthestageforcreatingtheseimageshere, ratherthantoexpandalimitedmodellater. computer-generatedimagesaresyntheticorartificial,inthesensethattheob- jects being imaged may not exist physically. in this chapter, we argue that the pre- ferredmethodtoformcomputer-generatedimagesissimilartotraditionalimaging methods, such as cameras and the human visual system. hence, before we discuss themechanicsofwritingprogramstogenerateimages,wediscussthewayimagesare formedbyopticalsystems.weconstructamodeloftheimage-formationprocessthat wecanthenusetounderstandanddevelopcomputer-generatedimagingsystems. in this chapter, we make minimal use of mathematics. we want to establish a paradigmforcreatingimagesandtopresentacomputerarchitectureforimplement- ingthatparadigm.detailsarepresentedinsubsequentchapters,wherewewillderive therelevantequations. 1.3.1 objects and viewers weliveinaworldofthree-dimensionalobjects.thedevelopmentofmanybranches of mathematics, including geometry and trigonometry, was in response to the de- siretosystematizeconceptuallysimpleideas,suchasthemeasurementofthesizeof objectsandthedistancebetweenobjects.oftenweseektorepresentourunderstand- ing of such spatial relationships with pictures or images, such as maps, paintings, and photographs. likewise, the development of many physical devicesincluding cameras,microscopes,andtelescopeswastiedtothedesiretovisualizespatialre- lationshipsamongobjects.hence,therealwayshasbeenafundamentallinkbetween thephysicsandthemathematicsofimageformationonethatwecanexploitinour developmentofcomputerimageformation. two basic entities must be part of any image-formation process, be it mathe- maticalorphysical:object andviewer.theobjectexistsinspaceindependentofany image-formation process and of any viewer. in computer graphics, where we deal withsyntheticobjects,weformobjectsbyspecifyingthepositionsinspaceofvarious geometricprimitives,suchaspoints,lines,andpolygons.inmostgraphicssystems, asetoflocationsinspace,orofvertices,issufficienttodefine,orapproximate,most objects.forexample,alinecanbespecifiedbytwovertices;apolygoncanbespecified byanorderedlistofvertices;andaspherecanbespecifiedbytwoverticesthatgiveits centerandanypointonitscircumference.oneofthemainfunctionsofacadsys- temistoprovideaninterfacethatmakesiteasyforausertobuildasyntheticmodel oftheworld.inchapter2,weshowhowopenglallowsustobuildsimpleobjects; inchapter11,welearntodefineobjectsinamannerthatincorporatesrelationships amongobjects. 12 chapter1 graphicssystemsandmodels c b (a) (b) (c) figure1.6 imageseenbythreedifferentviewers.(a)asview.(b)bs view.(c)csview. everyimagingsystemmustprovideameansofformingimagesfromobjects.to formanimage,wemusthavesomeoneorsomethingthatisviewingourobjects,beit aperson,acamera,oradigitizer.itistheviewerthatformstheimageofourobjects. inthehumanvisualsystem,theimageisformedonthebackoftheeye.inacamera, the image is formed in the film plane. it is easy to confuse images and objects. we usuallyseeanobjectfromoursingleperspectiveandforgetthatotherviewers,located inotherplaces,willseethesameobjectdifferently.figure1.6(a)showstwoviewers observingthesamebuilding.thisimageiswhatisseenbyanobserverawhoisfar enoughawayfromthebuildingtoseeboththebuildingandthetwootherviewers, bandc.fromasperspective,bandcappearasobjects,justasthebuildingdoes. figures1.6(b)and(c)showtheimagesseenbybandc,respectively.allthreeimages containthesamebuilding,buttheimageofthebuildingisdifferentinallthree. figure1.7 camerasystem. figure1.7showsacamerasystemviewingabuilding.herewecanobservethat both the object and the viewer exist in a three-dimensional world. however, the image that they definewhat we find on the film planeis two dimensional. the processbywhichthespecificationoftheobjectiscombinedwiththespecificationof the viewer to produce a two-dimensional image is the essence of image formation, andwewillstudyitindetail. 1.3.2 light and images theprecedingdescriptionofimageformationisfarfromcomplete.forexample,we haveyettomentionlight.iftherewerenolightsources,theobjectswouldbedark, andtherewouldbenothingvisibleinourimage.norhaveweindicatedhowcolor entersthepictureorwhattheeffectsofthesurfacepropertiesoftheobjectsare. takingamorephysicalapproach,wecanstartwiththearrangementshownin figure1.8, whichshowsasimplephysicalimagingsystem.again, weseeaphysical objectandaviewer(thecamera);now,however,thereisalightsourceinthescene. light from the source strikes various surfaces of the object, and a portion of the reflected light enters the camera through the lens. the details of the interaction between light and the surfaces of the object determine how much light enters the camera. 1.3 images:physicalandsynthetic 13 figure1.8 acamerasystemwithanobjectandalightsource. x rays light radio (cid:2)(nm) blue green red 350 (cid:2)(nm) 780 figure1.9 theelectromagneticspectrum. lightisaformofelectromagneticradiation.electromagneticenergytravelsas wavesthatcanbecharacterizedbyeithertheirwavelengthsortheirfrequencies.3the electromagnetic spectrum (figure 1.9) includes radio waves, infrared (heat), and a portionthatcausesaresponseinourvisualsystems.thisvisiblespectrum, which haswavelengthsintherangeof350to780nanometers(nm),iscalled(visible)light. a given light source has a color determined by the energy that it emits at various wavelengths. wavelengths in the middle of the range, around 520 nm, are seen as green;thosenear450nmareseenasblue;andthosenear650nmareseenasred.just aswitharainbow,lightatwavelengthsbetweenredandgreenweseeasyellow,and wavelengthsshorterthanbluegeneratevioletlight. lightsourcescanemitlighteitherasasetofdiscretefrequenciesorcontinuously. alaser,forexample,emitslightatasinglefrequency,whereasanincandescentlamp 3.therelationshipbetweenfrequency(f)andwavelength()isfc,wherecisthespeedoflight. 14 chapter1 graphicssystemsandmodels emitsenergyoverarangeoffrequencies.fortunately,incomputergraphics,except forrecognizingthatdistinctfrequenciesarevisibleasdistinctcolors,werarelyneed todealwiththephysicalpropertiesoflight,suchasitswavenature. instead,wecanfollowamoretraditionalpaththatiscorrectwhenweareoperat- ingwithsufficientlyhighlightlevelsandatascalewherethewavenatureoflightisnot asignificantfactor.geometricopticsmodelslightsourcesasemittersoflightenergy, eachofwhichhaveafixedintensity.modeledgeometrically,lighttravelsinstraight lines,fromthesourcestothoseobjectswithwhichitinteracts.anidealpointsource emitsenergyfromasinglelocationatoneormorefrequenciesequallyinalldirec- tions.morecomplexsources, suchasalightbulb, canbecharacterizedasemitting lightoveranareaandbyemittingmorelightinonedirectionthananother.apartic- ularsourceischaracterizedbytheintensityoflightthatitemitsateachfrequencyand bythatlightsdirectionality.weconsideronlypointsourcesfornow.morecomplex sources often can be approximated by a number of carefully placed point sources. modelingoflightsourcesisdiscussedinchapter6. 1.3.3 image formation models there are multiple approaches to how we can form images from a set of objects, thelight-reflectingpropertiesoftheseobjects,andthepropertiesofthelightsources inthescene.inthissection, weintroducetwophysicalapproaches.althoughthese approachesarenotsuitableforthereal-timegraphicsthatweultimatelywant,they willgiveusinsightintohowwecanbuildausefulimagingarchitecture. wecanstartbuildinganimagingmodelbyfollowinglightfromasource.con- siderthesceneillustratedinfigure1.10;itisilluminatedbyasinglepointsource.we includetheviewerinthefigurebecauseweareinterestedinthelightthatreachesher eye.theviewercanalsobeacamera,asshowninfigure1.11.arayisasemi-infinite linethatemanatesfromapointandtravelstoinfinityinaparticulardirection.be- causelighttravelsinstraightlines,wecanthinkintermsofraysoflightemanatingin alldirectionsfromourpointsource.aportionoftheseinfiniterayscontributestothe figure1.10 scenewithasinglepointlightsource. 1.3 images:physicalandsynthetic 15 b c d a figure 1.11 ray interactions. ray a enters camera directly. ray b goesofftoinfinity.raycisreflectedbyamirror.raydgoesthrougha transparentsphere. imageonthefilmplaneofourcamera.forexample,ifthesourceisvisiblefromthe camera,someoftheraysgodirectlyfromthesourcethroughthelensofthecamera, andstrikethefilmplane.mostrays,however,goofftoinfinity,neitherenteringthe cameradirectlynorstrikinganyoftheobjects.theserayscontributenothingtothe image, althoughtheymaybeseenbysomeotherviewer.theremainingraysstrike and illuminate objects. these rays can interact with the objects surfaces in a vari- etyofways.forexample,ifthesurfaceisamirror,areflectedraymightdepending ontheorientationofthesurfaceenterthelensofthecameraandcontributetothe image.othersurfacesscatterlightinalldirections.ifthesurfaceistransparent,the light ray from the source can pass through it and may interact with other objects, enterthecamera,ortraveltoinfinitywithoutstrikinganothersurface.4figure1.11 showssomeofthepossibilities. raytracingandphotonmappingareimage-formationtechniquesthatarebased ontheseideasandthatcanformthebasisforproducingcomputer-generatedimages. wecanusetheray-tracingideatosimulatephysicaleffectsascomplexaswewish,as longaswearewillingtocarryouttherequisitecomputing.althoughtracingrayscan provideacloseapproximationtothephysicalworld,itisnotwellsuitedforreal-time computation. other physical approaches to image formation are based on conservation of energy.themostimportantincomputergraphicsisradiosity.thismethodworks 4.thismodelofimageformationwasusedbyleonardodavinci500yearsago. 16 chapter1 graphicssystemsandmodels bestforsurfacesthatscattertheincominglightequallyinalldirections.eveninthis case,radiosityrequiresmorecomputationthancanbedoneinrealtime. giventhetimeconstraintsinimageformationbyreal-timegraphics,weusually aresatisfiedwithimagesthatlookreasonableratherthanimagesthatarephysically correct.suchisnotthecasewithimagesthatarenonreal-time,suchasthosegen- eratedforafeaturefilm,whichcanusehoursofcomputertimeforeachframe.with theincreasedspeedofpresenthardware,wecangetclosertophysicallycorrectimages inreal-timesystems.consequently,wewillreturntotheseapproachesinchapter13. 1.4 imaging systems wenowintroducetwophysicalimagingsystems:thepinholecameraandthehuman visualsystem.thepinholecameraisasimpleexampleofanimagingsystemthatwill enableustounderstandthefunctioningofcamerasandofotheropticalimagers.we emulateittobuildamodelofimageformation.thehumanvisualsystemisextremely complexbutstillobeysthephysicalprinciplesofotheropticalimagingsystems.we introduce it not only as an example of an imaging system but also because under- standing its properties will help us to exploit the capabilities of computer-graphics systems. 1.4.1 the pinhole camera thepinholecamerashowninfigure1.12providesanexampleofimageformation thatwecanunderstandwithasimplegeometricmodel.apinholecameraisabox withasmallholeinthecenterofonesideofthebox;thefilmisplacedinsidethebox onthesideoppositethepinhole.initially,thepinholeiscovered.itisuncoveredfor ashorttimetoexposethefilm.supposethatweorientourcameraalongthez-axis, withthepinholeattheoriginofourcoordinatesystem.weassumethattheholeis sosmallthatonlyasinglerayoflight,emanatingfromapoint,canenterit.thefilm planeislocatedadistancedfromthepinhole.asideview(figure1.13)allowsusto y x (x,y,z) z (x,y,z ) p p p d figure1.12 pinholecamera. 1.4 imagingsystems 17 y (y,z) z (y,(cid:3)d) p d figure1.13 sideviewofpinholecamera. calculatewheretheimageofthepoint (x,y,z)isonthefilmplane zd.using the fact that the two triangles shown in figure 1.13 are similar, we find that the y coordinateoftheimageisaty ,where p y y  . p zd asimilarcalculation,usingatopview,yields x x  . p zd the point (x ,y ,d) is called the projection of the point (x,y,z). note that all p p pointsalongthelinebetween(x,y,z)and(x ,y ,d)projectto(x ,y ,d)sothat p p p p wecannotgobackwardfromapointintheimageplanetothepointthatproduced it.inouridealizedmodel,thecoloronthefilmplaneatthispointwillbethecolorof thepoint(x,y,z).thefield,orangle,ofviewofourcameraistheanglemadebythe largestobjectthatourcameracanimageonitsfilmplane.wecancalculatethefield ofviewwiththeaidoffigure1.14.5ifhistheheightofthecamera,thentheangleof view is  2tan 1 h . 2d theidealpinholecamerahasaninfinitedepthoffield:everypointwithinitsfield ofviewisinfocus,regardlessofhowfaritisfromthecamera.theimageofapoint isapoint.thepinholecamerahastwodisadvantages.first, becausethepinholeis sosmallitadmitsonlyasinglerayfromapointsourcealmostnolightentersthe camera.second,thecameracannotbeadjustedtohaveadifferentangleofview. 5.ifweconsidertheprobleminthree,ratherthantwo,dimensions,thenthediagonallengthofthe filmwillsubstituteforh. 18 chapter1 graphicssystemsandmodels y (cid:4) h z d figure1.14 angleofview. thejumptomoresophisticatedcamerasandtootherimagingsystemsthathave lensesisdirect.byreplacingthepinholewithalens,wesolvethetwoproblemsofthe pinholecamera.first,thelensgathersmorelightthancanpassthroughthepinhole. the larger the aperture of the lens, the more light the lens can collect. second, by pickingalenswiththeproperfocallengthaselectionequivalenttochoosingdfor thepinholecamerawecanachieveanydesiredangleofview(upto180degrees). physicallenses,however,donothaveaninfinitedepthoffield:notallobjectsinfront ofthelensareinfocus. for our purposes, in this chapter, we can work with a pinhole camera whose cornea retina focallengthisthedistancedfromthefrontofthecameratothefilmplane.likethe pinholecamera,computergraphicsproducesimagesinwhichallobjectsareinfocus. lens iris rods and cones 1.4.2 the human visual system ourextremelycomplexvisualsystemhasallthecomponentsofaphysicalimaging optic nerve system, such as a camera or a microscope. the major components of the visual figure 1.15 the human systemareshowninfigure1.15.lightenterstheeyethroughthelensandcornea, visualsystem. atransparentstructurethatprotectstheeye.theirisopensandclosestoadjustthe amount of light entering the eye. the lens forms an image on a two-dimensional structure called the retina at the back of the eye. the rods and cones (so named because of their appearance when magnified) are light sensors and are located on theretina.theyareexcitedbyelectromagneticenergyintherangeof350to780nm. therodsarelow-level-lightsensorsthataccountforournightvisionandarenot colorsensitive; theconesareresponsibleforourcolorvision.thesizesoftherods andcones,coupledwiththeopticalpropertiesofthelensandcornea,determinethe resolutionofourvisualsystems,orourvisualacuity.resolutionisameasureofwhat sizeobjectswecansee.moretechnically,itisameasureofhowclosewecanplacetwo pointsandstillrecognizethattherearetwodistinctpoints. thesensorsinthehumaneyedonotreactuniformlytolightenergyatdifferent wavelengths.therearethreetypesofconesandasingletypeofrod.whereasintensity is a physical measure of light energy, brightness is a measure of how intense we 1.5 thesynthetic-cameramodel 19 perceivethelightemittedfromanobjecttobe.thehumanvisualsystemdoesnot have the same response to a monochromatic (single-frequency) red light as to a monochromatic green light. if these two lights were to emit the same energy, they would appear to us to have different brightness, because of the unequal response of the cones to red and green light. we are most sensitive to green light, and least sensitivetoredandblue. brightnessisanoverallmeasureofhowwereacttotheintensityoflight.human color-vision capabilities are due to the different sensitivities of the three types of cones.themajorconsequenceofhavingthreetypesofconesisthatinsteadofhaving toworkwithallvisiblewavelengthsindividually,wecanusethreestandardprimaries toapproximateanycolorthatwecanperceive.consequently,mostimage-production systems,includingfilmandvideo,workwithjustthreebasic,orprimary,colors.we discusscolorindepthinchapter2. theinitialprocessingoflightinthehumanvisualsystemisbasedonthesame principles used by most optical systems. however, the human visual system has a back end much more complex than that of a camera or telescope. the optic nerve is connected to the rods and cones in an extremely complex arrangement that has manyofthecharacteristicsofasophisticatedsignalprocessor.thefinalprocessing is done in a part of the brain called the visual cortex, where high-level functions, suchasobjectrecognition,arecarriedout.wewillomitanydiscussionofhigh-level processing;instead,wecanthinksimplyintermsofanimagethatisconveyedfrom therodsandconestothebrain. 1.5 the synthetic-camera model our models of optical imaging systems lead directly to the conceptual foundation formodernthree-dimensionalcomputergraphics.welookatcreatingacomputer- generatedimageasbeingsimilartoforminganimageusinganopticalsystem.this paradigmhasbecomeknownasthesynthetic-cameramodel.considertheimaging system shown in figure 1.16. again we see objects and a viewer. in this case, the viewer is a bellows camera.6 the image is formed on the film plane at the back of thecamera.sothatwecanemulatethisprocesstocreateartificialimages,weneedto identifyafewbasicprinciples. first, the specification of the objects is independent of the specification of the viewer.hence,weshouldexpectthat,withinagraphicslibrary,therewillbeseparate functionsforspecifyingtheobjectsandtheviewer. second,wecancomputetheimageusingsimplegeometriccalculations,justas wedidwiththepinholecamera.considerasideviewofacameraandasimpleobject, asshowninfigure1.17.theviewinpart(a)issimilartothatofthepinholecamera. 6. in a bellows camera, the front of the camera, where the lens is located, and the back of the camera,thefilmplane,areconnectedbyflexiblesides.thus,wecanmovethebackofthecamera independentlyofthefrontofthecamera,introducingadditionalflexibilityintheimage-formation process.weusethisflexibilityinchapter5. 20 chapter1 graphicssystemsandmodels figure1.16 imagingsystem. y y camera (y,z) (y,z) projector (y p ,d) object z z cop (y,(cid:3)d) p (a) (b) figure1.17 equivalentviewsofimageformation.(a)imageformedon thebackofthecamera.(b)imageplanemovedinfrontofthecamera. note that the image of the object is flipped relative to the object. whereas with a real camera, we would simply flip the film to regain the original orientation of the object,withoursyntheticcamerawecanavoidtheflippingbyasimpletrick.wedraw anotherplaneinfrontofthelens(figure1.17(b)),andworkinthreedimensions,as showninfigure1.18.wefindtheimageofapointontheobjectonthevirtualimage planebydrawingaline,calledaprojector,fromthepointtothecenterofthelens,or thecenterofprojection(cop).notethatallprojectorsareraysemanatingfromthe centerofprojection.inoursyntheticcamera, thevirtualimageplanethatwehave movedinfrontofthelensiscalledtheprojectionplane.theimageofthepointis located where the projector passes through the projection plane. in chapter 5, we discussthisprocessindetailandderivetherelevantmathematicalformulas. wemustalsoconsiderthelimitedsizeoftheimage.aswesaw, notallobjects canbeimagedontothepinholecamerasfilmplane.theangleofviewexpressesthis limitation.inthesyntheticcamera,wecanmovethislimitationtothefrontbyplac- ingaclippingrectangle,orclippingwindow,intheprojectionplane(figure1.19). thisrectangleactsasawindow,throughwhichaviewer,locatedatthecenterofpro- jection, sees the world. given the location of the center of projection, the location 1.6 theprogrammersinterface 21 figure1.18 imagingwiththesyntheticcamera. (a) (b) figure1.19 clipping.(a)windowininitialposition.(b)windowshifted. andorientationoftheprojectionplane,andthesizeoftheclippingrectangle,wecan determinewhichobjectswillappearintheimage. 1.6 the programmers interface there are numerous ways that a user can interact with a graphics system. with completelyself-containedpackages,suchastheonesusedinthecadcommunity,a userdevelopsimagesthroughinteractionswiththedisplayusinginputdevices,such as a mouse and a keyboard. in a typical application, such as the painting program showninfigure1.20,theuserseesmenusandiconsthatrepresentpossibleactions. 22 chapter1 graphicssystemsandmodels figure1.20 interfaceforapaintingprogram. keyboard graphics application library drivers mouse program (api) display figure1.21 applicationprogrammersmodelofgraphicssystem. byclickingontheseitems,theuserguidesthesoftwareandproducesimageswithout havingtowriteprograms. of course, someone has to develop the code for these applications, and many ofus,despitethesophisticationofcommercialproducts,stillhavetowriteourown graphicsapplicationprograms(andevenenjoydoingso). the interface between an application program and a graphics system can be specified through a set of functions that resides in a graphics library. these speci- ficationsarecalledtheapplicationprogrammersinterface(api).theapplication programmersmodelofthesystemisshowninfigure1.21.theapplicationprogram- merseesonlytheapiandisthusshieldedfromthedetailsofboththehardwareand thesoftwareimplementationofthegraphicslibrary.thesoftwaredriversarerespon- sibleforinterpretingtheoutputoftheapiandconvertingthesedatatoaformthat is understood by the particular hardware. from the perspective of the writer of an 1.6 theprogrammersinterface 23 applicationprogram,thefunctionsavailablethroughtheapishouldmatchthecon- ceptualmodelthattheuserwishestoemploytospecifyimages.7 1.6.1 the pen-plotter model historically,mostearlygraphicssystemsweretwo-dimensionalsystems.theconcep- tualmodelthattheyusedisnowreferredtoasthepen-plottermodel,referencingthe outputdevicethatwasavailableonthesesystems.apenplotter(figure1.22)pro- figure1.22 penplotter. ducesimagesbymovingapenheldbyagantry,astructurethatcanmovethepenin twoorthogonaldirectionsacrossthepaper.theplottercanraiseandlowerthepenas requiredtocreatethedesiredimage.penplottersarestillinuse;theyarewellsuited for drawing large diagrams, such as blueprints. various apissuch as logo and postscripthavetheiroriginsinthismodel.althoughtheydifferfromoneanother, theyhaveacommonviewoftheprocessofcreatinganimageasbeingsimilartothe processofdrawingonapadofpaper.theuserworksonatwo-dimensionalsurface ofsomesize.shemovesapenaroundonthissurface,leavinganimageonthepaper. wecandescribesuchagraphicssystemwiththefollowingdrawingfunctions: moveto(x,y) lineto(x,y) executionofthemovetofunctionmovesthepentothelocation(x,y)onthepaper withoutleavingamark.thelinetofunctionmovesthepento(x,y)anddrawsa line from the old to the new location of the pen. once we add a few initialization andterminationprocedures,aswellastheabilitytochangepenstoalterthedrawing colororlinethickness,wehaveasimplebutcompletegraphicssystem.hereisa fragmentofasimpleprograminsuchasystem: moveto(0, 0); lineto(1, 0); lineto(1, 1); lineto(0, 1); lineto(0,0); thisfragmentwouldgeneratetheoutputshowninfigure1.23(a).ifweaddedthe code moveto(0, 1); lineto(0.5, 1.866); lineto(1.5, 1.866); lineto(1.5, 0.866); 7.theremaybeoneormoreadditionallayersbetweentheapiandthedriver,suchasavirtual machineorhardwareabstractionlayer.however,becausetheapplicationprogrammerseesonlythe api,shedoesnotneedtoknowthislevelofdetail. 24 chapter1 graphicssystemsandmodels lineto(1, 0); moveto(1, 1); lineto(1,5,1.866); we would have the image of a cube formed by an oblique projection, as shown in figure1.23(b). (a) for certain applications, such as page layout in the printing industry, systems builtonthismodelworkwell.forexample,thepostscriptpage-descriptionlanguage, asophisticatedextensionoftheseideas,isastandardforcontrollingtypesettersand printers. an alternate raster-based, but still limiting, two-dimensional model relies on writingpixelsdirectlyintoaframebuffer.suchasystemcouldbebasedonasingle functionoftheform writepixel(x, y, color) (b) wherex,yisthelocationofthepixelintheframebufferandcolorgivesthecolor figure1.23 outputofpen- tobewrittenthere.suchmodelsarewellsuitedtowritingthealgorithmsforrasteri- plotter program for (a) a zationandprocessingofdigitalimages. square, and (b) a projection we are much more interested, however, in the three-dimensional world. the ofacube. pen-plottermodeldoesnotextendwelltothree-dimensionalgraphicssystems.for example, if we wish to use the pen-plotter model to produce the image of a three- dimensionalobjectonourtwo-dimensionalpad,eitherbyhandorbycomputer,then wehavetofigureoutwhereonthepagetoplacetwo-dimensionalpointscorrespond- ing to points on our three-dimensional object. these two-dimensional points are, aswesawinsection1.5, theprojectionsofpointsinthree-dimensionalspace.the mathematicalprocessofdeterminingprojectionsisanapplicationoftrigonometry. we develop the mathematics of projection in chapter 5; understanding projection is crucial to understanding three-dimensional graphics. we prefer, however, to use anapithatallowsuserstoworkdirectlyinthedomainoftheirproblemsandtouse computerstocarryoutthedetailsoftheprojectionprocessautomatically,withoutthe usershavingtomakeanytrigonometriccalculationswithintheapplicationprogram. thatapproachshouldbeaboontouserswhohavedifficultylearningtodrawvarious projectionsonadraftingboardorsketchingobjectsinperspective.moreimportant, userscanrelyonhardwareandsoftwareimplementationsofprojectionswithinthe implementationoftheapithatarefarmoreefficientthananypossibleimplementa- tionofprojectionswithintheirprogramswouldbe. 1.6.2 three-dimensional apis thesynthetic-cameramodelisthebasisforallthepopularapis,includingopengl, direct3d,andopenscenegraph.ifwearetofollowthesynthetic-cameramodel,we needfunctionsintheapitospecifythefollowing: objects aviewer 1.6 theprogrammersinterface 25 lightsources y materialproperties objects are usually defined by sets of vertices. for simple geometric objects b such as line segments, rectangles, and polygonsthere is a simple relationship be- tween a list of vertices, or positions in space, and the object. for more complex objects,theremaybemultiplewaysofdefiningtheobjectfromasetofvertices.acir- cle,forexample,canbedefinedbythreepointsonitscircumference,orbyitscenter andonepointonthecircumference. a mostapisprovidesimilarsetsofprimitiveobjectsfortheuser.theseprimitives x are usually those that can be displayed rapidly on the hardware. the usual sets in- cludepoints,linesegments,polygons,andsometimestext.openglprogramsdefine c primitives through lists of vertices. the following opengl code fragment specifies z thetriangularpolygonshowninfigure1.24throughfivefunctioncalls: figure1.24 atriangle. glbegin(glpolygon); glvertex3f(0.0, 0.0, 0.0);  vertex a  glvertex3f(0.0, 1.0, 0.0);  vertex b  glvertex3f(0.0, 0.0, 1.0);  vertex c  glend( ); thefunctionglbeginspecifiesthetypeofprimitivethattheverticesdefine.each subsequentexecutionofglvertex3fspecifiesthex,y,z coordinatesofalocation inspace.thefunctionglendendsthelistofvertices.notethatbyaddingadditional vertices, we can define an arbitrary polygon. if we change the type parameter, gl polygon, we can use the same vertices to define a different geometric primitive. for example, the type gllinestrip uses the vertices to define two connected line segments, whereas the type glpoints uses the same vertices to define three w points. cop someapislettheuserworkdirectlyintheframebufferbyprovidingfunctions that read and write pixels. some apis provide curves and surfaces as primitives; h often,however,thesetypesareapproximatedbyaseriesofsimplerprimitiveswithin the application program. opengl provides access to the frame buffer, curves, and surfaces. wecanspecifyaviewerorcamerainavarietyofways.availableapisdifferin how much flexibility they provide in camera selection and in how many different methodstheyallow.ifwelookatthecamerashowninfigure1.25,wecanidentify figure1.25 camera fourtypesofnecessaryspecifications: specification. 1. position thecameralocationusuallyisgivenbythepositionofthecenter ofthelens,whichisthecenterofprojection(cop). 2. orientation once we have positioned the camera, we can place a camera coordinate system with its origin at the center of projection. we can then rotatethecameraindependentlyaroundthethreeaxesofthissystem. 3. focallength thefocallengthofthelensdeterminesthesizeoftheimage onthefilmplaneor,equivalently,theportionoftheworldthecamerasees. 26 chapter1 graphicssystemsandmodels 4. filmplane thebackofthecamerahasaheightandawidth.onthebellows camera,andinsomeapis,theorientationofthebackofthecameracanbe adjustedindependentlyoftheorientationofthelens. these specifications can be satisfied in various ways. one way to develop the specifications for the camera location and orientation uses a series of coordinate- systemtransformations.thesetransformationsconvertobjectpositionsrepresented inacoordinatesystemthatspecifiesobjectverticestoobjectpositionsinacoordinate systemcenteredatthecop.thisapproachisuseful,bothforimplementingandfor gettingthefullsetofviewsthataflexiblecameracanprovide.weusethisapproach extensively,startinginchapter5. havingmanyparameterstoadjust, however, canalsomakeitdifficulttogeta desired image. part of the problem lies with the synthetic-camera model. classical viewing techniques, such as the ones used in architecture, stress the relationship betweentheobjectandtheviewer,ratherthantheindependence thatthesynthetic- cameramodelemphasizes.thus,theclassicaltwo-pointperspectiveofacubeshown infigure1.26isatwo-point perspectivebecauseofaparticularrelationshipbetween theviewerandtheplanesofthecube(seeexercise1.7).althoughtheopenglapi allowsustosettransformationswithcompletefreedom,italsoprovideshelpfulextra functions.forexample,considerthefollowingfunctioncalls: glulookat(copx, copy, copz, atx, aty, atz, upx, upy, upz); glperspective(fieldofview, aspectratio, near, far); thefirstfunctioncallpointsthecamerafromacenterofprojectiontowardadesired point(theat point),withaspecifiedupdirectionforthecamera.thesecondselects a lens for a perspective view (the field of view) and how much of the world that thecamerashouldimage(theaspectratioandthenear andfar distances).however, noneoftheapisbuiltonthesynthetic-cameramodelprovidefunctionsfordirectly specifyingadesiredrelationshipbetweenthecameraandanobject. light sources are defined by their location, strength, color, and directionality. apisprovideasetoffunctionstospecifytheseparametersforeachsource.material properties are characteristics, or attributes, of the objects, and such properties are specified through a series of function calls at the time that each object is defined. both light sources and material properties depend on the models of lightmaterial interactionssupportedbytheapi.wediscusssuchmodelsinchapter6. figure1.26 two-pointperspectiveofacube. 1.6 theprogrammersinterface 27 1.6.3 a sequence of images inchapter2,webeginourdetaileddiscussionoftheopenglapithatwewilluse throughoutthisbook.theimagesdefinedbyyouropenglprogramswillbeformed automaticallybythehardwareandsoftwareimplementationoftheimage-formation process. herewelookatasequenceofimagesthatshowswhatwecancreateusingthe opengl api. we present these images as an increasingly more complex series of renderings of the same objects. the sequence not only loosely follows the order in whichwepresentrelatedtopicsbutalsoreflectshowgraphicssystemshavedeveloped overthepast30years. colorplate1showsanimageofanartistscreationofasun-likeobject.color plate2showstheobjectrenderedusingonlylinesegments.althoughtheobjectcon- sistsofmanyparts,andalthoughtheprogrammermayhaveusedsophisticateddata structures to model each part and the relationships among the parts, the rendered objectshowsonlytheoutlinesoftheparts.thistypeofimageisknownasawire- frameimagebecausewecanseeonlytheedgesofsurfaces:suchanimagewouldbe producediftheobjectswereconstructedwithstiffwiresthatformedaframewithno solid material between the edges. before raster-graphics systems became available, wireframe images were the only type of computer-generated images that we could produce. incolorplate3,thesameobjecthasbeenrenderedwithflatpolygons.certain surfacesarenotvisiblebecausethereisasolidsurfacebetweenthemandtheviewer; these surfaces have been removed by a hidden-surfaceremoval (hsr) algorithm. most raster systems can fill the interior of polygons with a solid color in approxi- matelythesametimethattheycanrenderawireframeimage.althoughtheobjects arethreedimensional,eachsurfaceisdisplayedinasinglecolor,andtheimagefailsto showthethree-dimensionalshapesoftheobjects.earlyrastersystemscouldproduce imagesofthisform. in chapters 2 and 3, we will show you how to generate images composed of simple geometric objectspoints, line segments, and polygons. in chapters 4 and 5,youwilllearnhowtotransformobjectsinthreedimensionsandhowtoobtaina desiredthree-dimensionalviewofamodel,withhiddensurfacesremoved. color plate 4 illustrates smooth shading of the polygons that approximate the object; it shows that the object is three dimensional and gives the appearance of a smoothsurface.wedevelopshadingmodelsthataresupportedbyopenglinchap- ter6.theseshadingmodelsarealsosupportedinthehardwareofmostrecentwork- stations; generating the shaded image on one of these systems takes approximately thesameamountoftimeasdoesgeneratingawireframeimage. color plate 5 shows a more sophisticated wireframe model constructed using nurbssurfaces,whichweintroduceinchapter12.suchsurfacesgivetheapplica- tion programmer great flexibility in the design process but are ultimately rendered usinglinesegmentsandpolygons. in color plates 6 and 7, we add surface texture to our object; texture is one of the effects that we discuss in chapter 8. all recent graphics processors support 28 chapter1 graphicssystemsandmodels texturemappinginhardware,sorenderingofatexture-mappedimagerequireslittle additionaltime.incolorplate6,weuseatechniquecalledbumpmapping thatgives theappearanceofaroughsurfaceeventhoughwerenderthesameflatpolygonsasin theotherexamples.colorplate7showsanenvironmentmapappliedtothesurfaceof theobject,whichgivesthesurfacetheappearanceofamirror.thesetechniqueswill bediscussedindetailinchapters8and9. colorplate8showsasmallareaoftherenderingoftheobjectusinganenviron- mentmap.theimageontheleftshowsthejaggedartifactsknownasaliasingerrors thatareduetothediscretenatureoftheframebuffer.theimageontherighthasbeen renderedusingasmoothingorantialiasingmethodthatwewillstudyinchapters7 and8. notonlydotheseimagesshowwhatispossiblewithavailablehardwareanda goodapi,buttheyarealsosimpletogenerate,aswewillseeinsubsequentchapters. in addition, just as the images show incremental changes in the renderings, the programsareincrementallydifferentfromoneanother. 1.6.4 the modelingrendering paradigm inmanysituationsespeciallyincadapplicationsandinthedevelopmentofcom- pleximages,suchasformovieswecanseparatethemodelingofthescenefromthe productionoftheimage, ortherenderingofthescene.hence, wecanlookatim- ageformationasthetwo-stepprocessshowninfigure1.27.althoughthetasksare thesameasthosewehavebeendiscussing,thisblockdiagramsuggeststhatwemight implementthemodelerandtherendererwithdifferentsoftwareandhardware.for example,considertheproductionofasingleframeinananimation.wefirstwantto designandpositionourobjects.thisstepishighlyinteractive,andwedonotneed toworkwithdetailedimagesoftheobjects.consequently,weprefertocarryoutthis steponaninteractiveworkstationwithgoodgraphicshardware.oncewehavede- signedthescene,wewanttorenderit,addinglightsources,materialproperties,and avarietyofotherdetailedeffects,toformaproduction-qualityimage.thisstepre- quiresatremendousamountofcomputation,soweprefertouseahigh-performance clusterorarenderfarm.notonlyistheoptimalhardwaredifferentinthemodeling andrenderingsteps,butthesoftwarethatweusealsomaybedifferent. theinterfacebetweenthemodelerandrenderercanbeassimpleasafilepro- ducedbythemodelerthatdescribestheobjectsandthatcontainsadditionalinfor- mation important only to the renderer, such as light sources, viewer location, and materialproperties.pixarsrendermaninterfacefollowsthisapproachandusesafile interface file modeler renderer figure1.27 themodelingrenderingpipeline. 1.7 graphicsarchitectures 29 format that allows modelers to pass models to the renderer in text format. one of the other advantages of this approach is that it allows us to develop modelers that, althoughtheyusethesamerenderer,aretailoredtoparticularapplications.likewise, differentrendererscantakeasinputthesameinterfacefile.itisevenpossible,atleast inprinciple,todispensewiththemodelercompletelyandtouseastandardtexteditor togenerateaninterfacefile.foranybutthesimplestscenes,however,userscannot edit lists of information for a renderer. rather, they use interactive modeling soft- ware.becausewemusthaveatleastasimpleimageofourobjectstointeractwitha modeler,mostmodelersusethesynthetic-cameramodeltoproducetheseimagesin realtime. thisparadigmhasbecomepopularasamethodforgeneratingcomputergames andimagesovertheinternet.models,includingthegeometricobjects,lights,cam- eras,andmaterialproperties,areplacedinadatastructurecalledascenegraphthat ispassedtoarendereroragameengine.wewillexaminescenegraphsinchapter10. itisalsothestandardmethodusedintheanimationindustrywhereinteractivepro- grams such as maya and lightwave are used interactively to build characters using wireframesorunlitpolygons.finalrenderingcantakehoursperframe. 1.7 graphics architectures ononesideoftheapiistheapplicationprogram.ontheothersideissomecom- bination of hardware and software that implements the functionality of the api. researchers have taken various approaches to developing architectures to support graphicsapis. earlygraphicssystemsusedgeneral-purposecomputerswiththestandardvon neumannarchitecture.suchcomputersarecharacterizedbyasingleprocessingunit thatprocessesasingleinstructionatatime.asimplemodeloftheseearlygraphics systems is shown in figure 1.28. the display in these systems was based on a calli- graphiccrtdisplaythatincludedthenecessarycircuitrytogeneratealinesegment connectingtwopoints.thejobofthehostcomputerwastoruntheapplicationpro- gramandtocomputetheendpointsofthelinesegmentsintheimage(inunitsofthe display).thisinformationhadtobesenttothedisplayataratehighenoughtoavoid flickeronthedisplay.intheearlydaysofcomputergraphics,computersweresoslow thatrefreshingevensimpleimages,containingafewhundredlinesegments,would burdenanexpensivecomputer. digital host to analog figure1.28 earlygraphicssystem. 30 chapter1 graphicssystemsandmodels display host processor display list figure1.29 display-processorarchitecture. 1.7.1 display processors theearliestattemptstobuildspecial-purposegraphicssystemswereconcernedpri- marilywithrelievingthegeneral-purposecomputerfromthetaskofrefreshingthe displaycontinuously.thesedisplayprocessorshadconventionalarchitectures(fig- ure1.29)butincludedinstructionstodisplayprimitivesonthecrt.themainad- vantageofthedisplayprocessorwasthattheinstructionstogeneratetheimagecould be assembled once in the host and sent to the display processor, where they were storedinthedisplayprocessorsownmemoryasadisplaylist, ordisplayfile.the displayprocessorwouldthenexecuterepetitivelytheprograminthedisplaylist, at aratesufficienttoavoidflicker,independentlyofthehost,thusfreeingthehostfor othertasks.thisarchitecturehasbecomecloselyassociatedwiththeclientserverar- chitecturesthatwewilldiscussinchapter3. 1.7.2 pipeline architectures themajoradvancesingraphicsarchitecturescloselyparalleltheadvancesinwork- stations.inbothcases, theabilitytocreatespecial-purposevlsichipswasthekey enablingtechnologydevelopment.inaddition,theavailabilityofinexpensivesolid- statememoryledtotheuniversalityofrasterdisplays.forcomputer-graphicsappli- cations,themostimportantuseofcustomvlsicircuitshasbeenincreatingpipeline architectures. pipeliningissimilartoanassemblylineinacarplant.asthechassispassesdown the line, a series of operations is performed on it, each using specialized tools and workers,untilattheend,theassemblyprocessiscomplete.atanyonetime,multiple carsareunderconstructionandthereisasignificantdelayorlatencybetweenwhen achassisstartsdowntheassemblylineandthefinishedvehicleiscomplete.however, thenumberofcarsproducedinagiventime,thethroughput,ismuchhigherthanif asingleteambuilteachcar. the concept of pipelining is illustrated in figure 1.30 for a simple arithmetic calculation.inourpipeline,thereisanadderandamultiplier.ifweusethisconfigu- rationtocomputea(bc),thenthecalculationtakesonemultiplicationandone additionthe same amount of work required if we use a single processor to carry outbothoperations.however,supposethatwehavetocarryoutthesamecomputa- 1.7 graphicsarchitectures 31 a b  c  figure1.30 arithmeticpipeline. vertex clipper and fragment vertices rasterizer pixels processor primitive assembler processor figure1.31 geometricpipeline. tionwithmanyvaluesofa,b,andc.now,themultipliercanpassontheresultsofits calculationtotheadderandcanstartitsnextmultiplicationwhiletheaddercarries outthesecondstepofthecalculationonthefirstsetofdata.hence,whereasittakes thesameamountoftimetocalculatetheresultsforanyonesetofdata,whenweare workingontwosetsofdataatonetime,ourtotaltimeforcalculationisshortened markedly.heretherateatwhichdataflowsthroughthesystem,thethroughputofthe system,hasbeendoubled.notethatasweaddmoreboxestoapipeline,thelatency ofthesystemincreasesandwemustbalancelatencyagainstincreasedthroughputin evaluatingtheperformanceofapipeline. we can construct pipelines for more complex arithmetic calculations that will affordevengreaterincreasesinthroughput.ofcourse,thereisnopointinbuilding a pipeline unless we will do the same operation on many data sets. but that is just whatwedoincomputergraphics,wherelargesetsofverticesmustbeprocessedin thesamemanner. 1.7.3 the graphics pipeline westartwithasetofobjects.eachobjectcomprisesasetofgraphicalprimitives.each primitivecomprisesasetofvertices.wecanthinkofthecollectionofprimitivetypes andverticesasdefiningthegeometryofthescene.inacomplexscene,theremaybe thousandseven millionsof vertices that define the objects. we must process all theseverticesinasimilarmannertoformanimageintheframebuffer.ifwethinkin termsofprocessingthegeometryofourobjectstoobtainanimage,wecanemploy theblockdiagraminfigure1.31,whichshowsthefourmajorstepsintheimaging process: 1. vertexprocessing 2. clippingandprimitiveassembly 3. rasterization 4. fragmentprocessing 32 chapter1 graphicssystemsandmodels insubsequentchapters,wediscussthedetailsofthesesteps.herewearecontentto overviewthesestepsandshowthattheycanbepipelined. 1.7.4 vertex processing in the first block of our pipeline, each vertex is processed independently. the two major functions of this block are to carry out coordinate transformations and to computeacolorforeachvertex. manyofthestepsintheimagingprocesscanbeviewedastransformationsbe- tweenrepresentationsofobjectsindifferentcoordinatesystems.forexample,inthe syntheticcameraparadigm,amajorpartofviewingistoconverttoarepresentation ofobjectsfromthesysteminwhichtheyweredefinedtoarepresentationinterms ofthecoordinatesystemofthecamera.afurtherexampleofatransformationarises whenwefinallyputourimagesontotheoutputdevice.theinternalrepresentationof objectswhetherinthecameracoordinatesystemorperhapsinasystemusedbythe graphicssoftwareeventuallymustberepresentedintermsofthecoordinatesystem ofthedisplay.wecanrepresenteachchangeofcoordinatesystemsbyamatrix.we canrepresentsuccessivechangesincoordinatesystemsbymultiplying, orconcate- nating,theindividualmatricesintoasinglematrix.inchapter4,weexaminethese operationsindetail.becausemultiplyingonematrixbyanothermatrixyieldsathird matrix,asequenceoftransformationsisanobviouscandidateforapipelinearchitec- ture.inaddition,becausethematricesthatweuseincomputergraphicswillalways besmall(44),wehavetheopportunitytouseparallelismwithinthetransforma- tionblocksinthepipeline. eventually,aftermultiplestagesoftransformation,thegeometryistransformed byaprojectiontransformation.inchapter5,weseethatwecanimplementthisstep using 4  4 matrices, and thus projection fits in the pipeline. in general, we want to keep three-dimensional information as long as possible, as objects pass through thepipeline.consequently,theprojectiontransformationissomewhatmoregeneral than the projections in section 1.5. in addition to retaining three-dimensional in- formation,thereisavarietyofprojectionsthatwecanimplement.wewillseethese projectionsinchapter5. the assignment of vertex colors can be as simple as the program specifying a colororascomplexasthecomputationofacolorfromaphysicallyrealisticlighting model that incorporates the surface properties of the object and the characteristic lightsourcesinthescene.wewilldiscusslightingmodelsinchapter6. 1.7.5 clipping and primitive assembly the second fundamental block in the implementation of the standard graphics pipelineisforclippingandprimitiveassembly.wemustdoclippingbecauseofthe limitationthatnoimagingsystemcanseethewholeworldatonce.thehumanretina hasalimitedsizecorrespondingtoanapproximately90-degreefieldofview.cameras havefilmoflimitedsize,andwecanadjusttheirfieldsofviewbyselectingdifferent lenses. 1.8 programmablepipelines 33 weobtaintheequivalentpropertyinthesyntheticcamerabyconsideringaclip- pingvolume,suchasthepyramidinfrontofthelensinfigure1.18.theprojections of objects in this volume appear in the image. those that are outside do not and aresaidtobeclippedout.objectsthatstraddletheedgesoftheclippingvolumeare partlyvisibleintheimage.efficientclippingalgorithmsaredevelopedinchapter7. clippingmustbedoneonaprimitivebyprimitivebasisratherthanonavertex by vertex basis. thus, within this stage of the pipeline, we must assemble sets of verticesintoprimitives,suchaslinesegmentsandpolygons,beforeclippingcantake place.consequently,theoutputofthisstageisasetofprimitiveswhoseprojections canappearintheimage. 1.7.6 rasterization the primitives that emerge from the clipper are still represented in terms of their vertices and must be further processed to generate pixels in the frame buffer. for example, if three vertices specify a triangle filled with a solid color, the rasterizer mustdeterminewhichpixelsintheframebufferareinsidethepolygon.wediscuss this rasterization (or scan-conversion) process in chapter 8 for line segments and polygons. the output of the rasterizer is a set of fragments for each primitive. a fragment can be thought of as a potential pixel that carries with it information, including its color and location, that is used to update the corresponding pixel in theframebuffer.fragmentscanalsocarryalongdepthinformationthatallowslater stages to determine if a particular fragment lies behind other previously rasterized fragmentsforagivenpixel. 1.7.7 fragment processing thefinalblockinourpipelinetakesinthefragmentsgeneratedbytherasterizerand updatesthepixelsintheframebuffer.iftheapplicationgeneratedthree-dimensional data, some fragments may not be visible because the surfaces that they define are behind other surfaces. the color of a fragment may be altered by texture mapping or bump mapping as shown in color plates 6 and 7. the color of the pixel that correspondstoafragmentcanalsobereadfromtheframebufferandblendedwith the fragments color to create translucent effects. these effects will be covered in chapters8and9. 1.8 programmable pipelines graphics architectures have gone through multiple cycles in which the importance of special-purpose hardware relative to standard cpus has gone back and forth. however,theimportanceofthepipelinearchitecturehasremainedregardlessofthis cycle.noneoftheotherapproachesraytracing,radiosity,photonmappingleads toreal-timeperformance.hence, thecommoditygraphicsmarketisdominatedby graphicscardsthathavepipelinesbuiltintothegraphicsprocessingunit.allofthese 34 chapter1 graphicssystemsandmodels commodity cards implement the pipeline that we have just described, albeit with moreoptions,manyofwhichwewilldiscussinlaterchapters. formanyyears,thesepipelinearchitectureshadafixedfunctionality.although the application program could set many parameters, the basic operations available within the pipeline were fixed. for example, there was only one lighting model for howtocomputeashadeusingthespecifiedlightsourcesandmaterials. recently,therehasbeenamajoradvanceinpipelinearchitectures.boththever- tex processor and the fragment processor are now programable by the application program.oneofthemostexcitingaspectsofthisadvanceisthatmanyofthetech- niquesthatformerlycouldnotbedoneinrealtimebecausetheywerenotpartofthe fixed-functionpipelinecannowbedoneinrealtime. vertexprogramscanalterthelocationorcolorofeachvertexasitflowsthrough thepipeline.thus,wecanimplementavarietyoflight-materialmodelsorcreatenew kindsofprojections.fragmentprogramsallowustousetexturesinnewways.bump mapping, which is illustrated in color plate 6, is but one example of an algorithm thatisnowprogrammablethroughtexturemappingbutformerlycouldonlybedone off-line.chapter9isdevotedtothesenewmethodologies. 1.9 performance characteristics therearetwofundamentallydifferenttypesofprocessinginourpipelinearchitec- ture. at the front end, there is geometric processing, based on processing vertices throughthevarioustransformations,vertexshading,clipping,andprimitiveassem- bly.thisprocessingisideallysuitedforpipelining, anditusuallyinvolvesfloating- point calculations. the geometry engine developed by silicon graphics, inc. (sgi) wasavlsiimplementationformanyoftheseoperationsinaspecial-purposechip thatbecamethebasisforaseriesoffastgraphicsworkstations.later,floating-point accelerator chips put 4  4 matrix-transformation units on the chip, reducing a matrixmultiplicationtoasingleinstruction.today,graphicsworkstationsandcom- moditygraphicscardsusegraphicsprocessingunits(gpus)thatperformmostofthe graphicsoperationsatthechiplevel.pipelinearchitecturesarethedominanttypeof high-performancesystem. beginningwithrasterizationandincludingmanyfeaturesthatwediscusslater, processinginvolvesadirectmanipulationofbitsintheframebuffer.thisback-end processingisfundamentallydifferentfromfront-endprocessing,andweimplement it most effectively using architectures that have the ability to move blocks of bits quickly. the overall performance of a system is characterized by how fast we can move geometric entities through the pipeline and by how many pixels per second wecanalterintheframebuffer.consequently,thefastestgraphicsworkstationsare characterizedbyoneormoregeometricpipelinesatthefrontendsandparallelbit processors at the back ends. until about 10 years ago, there was a clear distinction between front- and back-end processing and there were different components and boardsdedicatedtoeach.now,commoditygraphicscardsusegpusthatcontainthe summaryandnotes 35 entirepipelinewithinasinglechip.thelatestcardsimplementtheentirepipelineus- ingfloating-pointarithmeticandhavefloating-pointframebuffers.thesegpusare sopowerfulthattheyarebeingusedforpurposesotherthangraphicsapplications. pipeline architectures dominate the graphics field, especially where real-time performance is of importance. our presentation has made a case for using such anarchitecturetoimplementthehardwareinasystem.commoditygraphicscards incorporatethepipelinewithintheirgpus.cardsthatcostlessthan100canrender millionsofshadedtexture-mappedpolygonspersecond.however,wecanalsomake asstrongacaseforpipeliningbeingthebasisofacompletesoftwareimplementation ofanapi.thepowerofthesynthetic-cameraparadigmisthatthelatterworkswell inbothcases. however,whererealismisimportant,othertypesofrendererscanperformbet- terattheexpenseofrequiringmorecomputationtime.pixarsrendermaninterface wascreatedtointerfacetheiroff-linerenderer.physicallybasedtechniques, suchas raytracingandradiosity,cancreatephotorealisticimageswithgreatfidelity,butnot inrealtime. summary and notes inthischapter,wesetthestageforourtop-downdevelopmentofcomputergraphics. wepresentedtheoverallpicturesothatyoucanproceedtowritinggraphicsapplica- tionsprogramsinchapter2withoutfeelingthatyouareworkinginavacuum. westressedthatcomputergraphicsisamethodofimageformationthatshould berelatedtoclassicalmethodsofimageformationinparticular,toimageformation inopticalsystems,suchasincameras.inadditiontoexplainingthepinholecamera, wehaveintroducedthehumanvisualsystem;bothareexamplesofimagingsystems. wedescribedmultipleimage-formationparadigms,eachofwhichhasapplica- bilityincomputergraphics.thesynthetic-cameramodelhastwoimportantconse- quencesforcomputergraphics.first,itstressestheindependenceoftheobjectsand thevieweradistinctionthatleadstoagoodwayoforganizingthefunctionsthat willbeinagraphicslibrary.second,itleadstothenotionofapipelinearchitecture, inwhicheachofthevariousstagesinthepipelineperformsdistinctoperationson geometricentities,thenpassesonthetransformedobjectstothenextstage. wealsointroducedtheideaoftracingraysoflighttoobtainanimage.thispara- digmisespeciallyusefulinunderstandingtheinteractionbetweenlightandmaterials thatisessentialtophysicalimageformation.becauseraytracingandotherphysically basedstrategiescannotrenderscenesinrealtime,wedeferfurtherdiscussionofthem untilchapter13. themodelingrenderingparadigmisbecomingincreasinglyimportant.astan- dard graphics workstation can generate millions of line segments or polygons per secondataresolutionofupto12801024pixels.suchaworkstationcanshadethe polygonsusingasimpleshadingmodelandcandisplayonlyvisiblesurfacesatthis rate.however,realisticimagesmayrequirearesolutionofupto40006000pixels